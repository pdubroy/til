# JAX

I was recently introduced to [JAX](https://github.com/google/jax):

> JAX is a Python library for accelerator-oriented array computation and program transformation, designed for high-performance numerical computing and large-scale machine learning.

My own use cases lean more towards numerical computing; in particular, I'm interested JAX's support for automatic differentiation and non-linear optimization. Below is a short example.

### Automatic differentiation

First, we'll define a couple functions. We'll use the equation for a parabola, since that's a nice, smooth function whose minimum we can easily compute ourselves.

```python
import jax.numpy as jnp
from jax import grad
from jax.scipy.optimize import minimize

def parabola(h, k, x):
    """Parabola with minimum at (h, k)."""
    return (x - h) ** 2 + k

def my_parabola(x):
    """Parabola with minimum at (2, 3)."""
    return parabola(2., 3., x)
```

We can use `grad` to use JAX's autodiff support to find the gradient of `my_parabola`. If we evaluate it at x=0, we should get a negative value, indicating that the function is decreasing at that point:

```python
my_parabola_grad = grad(my_parabola)
print(my_parabola_grad(0.)) # Prints -4.0
```

And if we evaluate it at x=3, we should get a positive value:

```python
print(my_parabola_grad(3.)) # Prints 2.0
```

We can also specify which parameters the function should be differentiated with respect to. For example, we can use this to differentiate `parabola` directly, and pass the values for `h` and `k` when we evaluate the gradient:

```python
parabola_grad = grad(parabola, argnums=2)
print(parabola_grad(2., 3., 0.)) # Prints 2.0
```

### Optimization

We can use `minimize`, from the `jax.scipy.optimize` package to find the minimum of the function. Note that parameters of the optimization problem are passed in a JAX array as the first argument to the function. Since our function has a different interface, we need to adapt it:

```python
f = lambda args_arr: my_parabola(args_arr[0])
results = minimize(f, jnp.array([0.]), method="BFGS")
print(results.x) # Prints [2.]
print(results.fun) # Prints 3.0 (the final return value of the function).
```

[BFGS][] is the only supported optimization method. For more methods, see [Optax][], which is a gradient processing and optimization library for JAX.

[BFGS]: https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm
[Optax]: https://github.com/google-deepmind/optax
